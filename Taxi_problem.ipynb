{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiFIRpZuZQHQ7LYCVrgodY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACPSYan/Georgia/blob/main/Taxi_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZGEU-Uo-x9s"
      },
      "outputs": [],
      "source": [
        "# value iteration\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
        "    total reward.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    policy: the policy to be used.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    returns:\n",
        "    total reward: real value of the total reward recieved by agent under policy.\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0,  n = 2000):\n",
        "    \"\"\" Evaluates a policy by running it n times.\n",
        "    returns:\n",
        "    average total reward\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "            run_episode(env, policy, gamma = gamma, render = False)\n",
        "            for _ in range(n)]\n",
        "    return scores\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "def value_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Value-iteration algorithm \"\"\"\n",
        "    v = np.zeros(env.nS)  # initialize value-function\n",
        "    max_iterations = 10000\n",
        "    eps = 1e-20\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)]\n",
        "            v[s] = max(q_sa)\n",
        "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
        "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
        "            break\n",
        "    return v\n",
        "\n",
        "gamma = [0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1.0]\n",
        "policy_average_score = []\n",
        "time_iteration=[]\n",
        "for i in gamma:\n",
        "    print(\"gamma = \" + str(i))\n",
        "    env_name  = 'Taxi-v3'\n",
        "    env = gym.make(env_name)\n",
        "    env.reset()\n",
        "    start = datetime.now()\n",
        "    optimal_v = value_iteration(env, i)\n",
        "    end = datetime.now()\n",
        "    time_iteration.append((end-start).total_seconds())\n",
        "    policy = extract_policy(optimal_v, i)\n",
        "    policy_scores = evaluate_policy(env, policy, i, n=3000)\n",
        "    policy_average_score.append(np.mean(policy_scores))\n",
        "\n",
        "    print('Policy average score = ', np.mean(policy_scores))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, xlabel = 'gamma', ylabel= 'Time')\n",
        "ax.plot(gamma, time_iteration, 'o-', color='r')\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, xlabel = 'gamma', ylabel= 'Policy_Score')\n",
        "ax.plot(gamma, policy_average_score, 'o-', color='b')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# policy iteration\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def run_episode(env, policy, gamma = 1.0, render = False):\n",
        "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
        "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
        "    return scores\n",
        "\n",
        "def extract_policy(v, gamma = 1.0):\n",
        "    \"\"\" Extract the policy given a value-function \"\"\"\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy\n",
        "\n",
        "def compute_policy_v(env, policy, gamma=1.0):\n",
        "    \"\"\" Iteratively evaluate the value-function under policy.\n",
        "    Alternatively, we could formulate a set of linear equations in iterms of v[s]\n",
        "    and solve them to find the value function.\n",
        "    \"\"\"\n",
        "    v = np.zeros(env.nS)\n",
        "    eps = 1e-10\n",
        "    #eps=0.1\n",
        "    while True:\n",
        "        prev_v = np.copy(v)\n",
        "        for s in range(env.nS):\n",
        "            policy_a = policy[s]\n",
        "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
        "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
        "            # value converged\n",
        "            break\n",
        "    return v\n",
        "\n",
        "def policy_iteration(env, gamma = 1.0):\n",
        "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
        "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
        "    max_iterations = 100\n",
        "    gamma = 1.0\n",
        "    for i in range(max_iterations):\n",
        "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
        "        new_policy = extract_policy(old_policy_v, gamma)\n",
        "        if (np.all(policy == new_policy)):\n",
        "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
        "            break\n",
        "        policy = new_policy\n",
        "    return policy\n",
        "\n",
        "gamma=1.0\n",
        "policy_average_score =0\n",
        "time_iteration=0\n",
        "print(\"gamma = \" + str(gamma))\n",
        "env_name  = 'Taxi-v3'\n",
        "env = gym.make(env_name)\n",
        "start = datetime.now()\n",
        "optimal_policy = policy_iteration(env, gamma =1.0)\n",
        "end = datetime.now()\n",
        "print((end-start).total_seconds())\n",
        "\n",
        "scores = evaluate_policy(env, optimal_policy, gamma=1.0)\n",
        "print('Policy average score = ', np.mean(scores))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ibFs7_7-9qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q-learning\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def main():\n",
        "\n",
        "    # create Taxi environment\n",
        "    env = gym.make('Taxi-v3')\n",
        "    start= datetime.now()\n",
        "\n",
        "    # initialize q-table\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "    # hyperparameters\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.8\n",
        "    epsilon = 1\n",
        "    decay_rate= 0.005\n",
        "\n",
        "    # training variables\n",
        "    num_episodes = 1000\n",
        "    max_steps = 99 # per episode\n",
        "\n",
        "    # training\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # reset the environment\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        for s in range(max_steps):\n",
        "\n",
        "            # exploration-exploitation tradeoff\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                # explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # exploit\n",
        "                action = np.argmax(qtable[state,:])\n",
        "\n",
        "            # take action and observe reward\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Q-learning algorithm\n",
        "            qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "            # Update to our new state\n",
        "            state = new_state\n",
        "\n",
        "            # if done, finish episode\n",
        "            if done == True:\n",
        "                break\n",
        "\n",
        "        # Decrease epsilon\n",
        "        epsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "    print(f\"Training completed over {num_episodes} episodes\")\n",
        "    end= datetime.now()\n",
        "    print((end-start).total_seconds())\n",
        "    input(\"Press Enter to watch trained agent...\")\n",
        "\n",
        "    # watch trained agent\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "\n",
        "    for s in range(max_steps):\n",
        "\n",
        "        print(f\"TRAINED AGENT\")\n",
        "        print(\"Step {}\".format(s+1))\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        rewards += reward\n",
        "        env.render()\n",
        "        print(f\"score: {rewards}\")\n",
        "        state = new_state\n",
        "\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHJ_ATt6_FgY",
        "outputId": "3eccbf35-32ee-4e41-e1df-0ff81526bc06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed over 1000 episodes\n",
            "1.734853\n",
            "Press Enter to watch trained agent...\n",
            "TRAINED AGENT\n",
            "Step 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: -1\n",
            "TRAINED AGENT\n",
            "Step 2\n",
            "score: -2\n",
            "TRAINED AGENT\n",
            "Step 3\n",
            "score: -3\n",
            "TRAINED AGENT\n",
            "Step 4\n",
            "score: -4\n",
            "TRAINED AGENT\n",
            "Step 5\n",
            "score: -5\n",
            "TRAINED AGENT\n",
            "Step 6\n",
            "score: -6\n",
            "TRAINED AGENT\n",
            "Step 7\n",
            "score: -7\n",
            "TRAINED AGENT\n",
            "Step 8\n",
            "score: -8\n",
            "TRAINED AGENT\n",
            "Step 9\n",
            "score: -9\n",
            "TRAINED AGENT\n",
            "Step 10\n",
            "score: -10\n",
            "TRAINED AGENT\n",
            "Step 11\n",
            "score: -11\n",
            "TRAINED AGENT\n",
            "Step 12\n",
            "score: 9\n"
          ]
        }
      ]
    }
  ]
}